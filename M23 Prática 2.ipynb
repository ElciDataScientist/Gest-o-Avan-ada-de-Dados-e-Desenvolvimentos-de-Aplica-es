{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Passo a passo para o algoritmo Random Forest:\n",
        "## **1) Bootstrap:**\n",
        "- Dado um conjunto de dados original de tamanho n\n",
        "- Crie m novos conjuntos de dados (amostras bootstrap) de tamanho n\n",
        "- Cada amostra é criada através de amostragem com reposição do conjunto original\n",
        "## **2) Construção das árvores:**\n",
        "- Para cada amostra bootstrap, construa uma árvore de decisão\n",
        "- Ao construir cada nó da árvore:\n",
        "- Selecione aleatoriamente k features (k < número total de features)\n",
        "- Escolha a melhor feature entre as k para dividir o nó\n",
        "- Cresça a árvore até a profundidade máxima ou até que um critério de parada seja atingido\n",
        "## **3) Agregação:**\n",
        "- Para fazer previsões, use todas as árvores construídas\n",
        "- Para classificação: use votação majoritária\n",
        "- Para regressão: calcule a média das previsões"
      ],
      "metadata": {
        "id": "9t-Afc4-_I66"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explicação do Random Forest:\n",
        "\n",
        "   Random Forest é um algoritmo de ensemble learning que combina múltiplas árvores de decisão para criar um modelo mais robusto e preciso. Imagine que você está tentando tomar uma decisão importante e pede conselhos a um grupo de amigos. Cada amigo tem acesso a informações ligeiramente diferentes (as amostras bootstrap) e considera apenas alguns aspectos aleatórios do problema de cada vez (a seleção aleatória de features). No final, você considera todos os conselhos e toma uma decisão baseada na opinião majoritária. Isso é essencialmente o que o Random Forest faz.\n",
        "\n",
        "##   **O \"random\" em Random Forest vem de dois elementos de aleatoriedade:**\n",
        " - 1) A criação de amostras bootstrap aleatórias para treinar cada árvore.\n",
        " - 2) A seleção aleatória de um subconjunto de features em cada divisão da\n",
        "   árvore.\n",
        "\n",
        "  Essa aleatoriedade ajuda a criar um conjunto diversificado de árvores, cada uma capturando diferentes padrões nos dados. Ao combinar essas árvores, o Random Forest pode produzir previsões mais estáveis e precisas, além de ser menos propenso ao overfitting em comparação com uma única árvore de decisão."
      ],
      "metadata": {
        "id": "ka8QANtN_nkS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Diferenças entre Bagging e Random Forest:\n",
        "## 1. **Tipo de modelo base:**\n",
        "- Bagging: Pode usar qualquer tipo de modelo como base (árvores de decisão, regressão linear, etc.).\n",
        "- Random Forest: Usa especificamente árvores de decisão como modelos base.\n",
        "## 2. **Seleção de features:**\n",
        "- Bagging: Usa todas as features disponíveis para cada modelo base.\n",
        "- Random Forest: Seleciona aleatoriamente um subconjunto de features em cada divisão da árvore.\n",
        "## 3. **Diversidade dos modelos:**\n",
        "- Bagging: A diversidade vem principalmente das diferentes amostras bootstrap.\n",
        "- Random Forest: A diversidade vem tanto das amostras bootstrap quanto da seleção aleatória de features.\n",
        "## **4. Correlação entre modelos:**\n",
        "- Bagging: Os modelos podem ser altamente correlacionados se usarem todas as features.\n",
        "- Random Forest: A seleção aleatória de features reduz a correlação entre as árvores.\n",
        "## **5. Interpretabilidade:**\n",
        "- Bagging: A interpretabilidade depende do modelo base escolhido.\n",
        "- Random Forest: Oferece medidas de importância de features, facilitando a interpretação.\n",
        "## **6. Overfitting:**\n",
        "Bagging: Reduz o overfitting, mas pode ainda ser suscetível se os modelos base forem complexos.\n",
        "Random Forest: Geralmente mais resistente ao overfitting devido à seleção aleatória de features.\n",
        "## **7. Aplicação:**\n",
        "Bagging: Mais genérico, pode ser aplicado a uma variedade de problemas e modelos.\n",
        "Random Forest: Específico para problemas onde árvores de decisão são apropriadas."
      ],
      "metadata": {
        "id": "89528zuNANdr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "P3PNXFsVBVFp"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleRandomForest:\n",
        "    def __init__(self, n_estimators=10, max_features='sqrt', max_depth=None):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_features = max_features\n",
        "        self.max_depth = max_depth\n",
        "        self.trees = []\n",
        "\n",
        "    def bootstrap_sample(self, X, y):\n",
        "        n_samples = X.shape[0]\n",
        "        idxs = np.random.choice(n_samples, size=n_samples, replace=True)\n",
        "        return X[idxs], y[idxs]\n",
        "\n",
        "    def get_random_features(self, X):\n",
        "        n_features = X.shape[1]\n",
        "        if self.max_features == 'sqrt':\n",
        "            n_random_features = int(np.sqrt(n_features))\n",
        "        elif self.max_features == 'log2':\n",
        "            n_random_features = int(np.log2(n_features))\n",
        "        else:\n",
        "            n_random_features = self.max_features\n",
        "        random_feature_idxs = np.random.choice(n_features, size=n_random_features, replace=False)\n",
        "        return random_feature_idxs\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.trees = []\n",
        "        for _ in range(self.n_estimators):\n",
        "            tree = DecisionTreeClassifier(max_features=self.max_features, max_depth=self.max_depth)\n",
        "            X_sample, y_sample = self.bootstrap_sample(X, y)\n",
        "            tree.fit(X_sample, y_sample)\n",
        "            self.trees.append(tree)\n",
        "\n",
        "    def predict(self, X):\n",
        "        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n",
        "        return np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=tree_preds)"
      ],
      "metadata": {
        "id": "I654bTvQBW1E"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemplo de uso\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "rf = SimpleRandomForest(n_estimators=100, max_features='sqrt', max_depth=10)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "predictions = rf.predict(X_test)\n",
        "accuracy = np.mean(predictions == y_test)\n",
        "print(f\"Acurácia: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOs_LAznBa5S",
        "outputId": "ed5d0300-fd1d-4f74-87fb-8383a3d63a1d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acurácia: 0.89\n"
          ]
        }
      ]
    }
  ]
}
