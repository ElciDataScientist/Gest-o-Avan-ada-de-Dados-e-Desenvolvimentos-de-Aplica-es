{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Passo a passo para o Bagging:\n",
        "## 1) **Bootstrap:**\n",
        "- Dado um conjunto de dados original de tamanho n\n",
        "- Crie m novos conjuntos de dados (amostras bootstrap) de tamanho n\n",
        "- Cada amostra é criada através de amostragem com reposição do conjunto original\n",
        "## 2) **Modelagem:**\n",
        "- Para cada amostra bootstrap, treine um modelo de aprendizado de máquina\n",
        "- Geralmente, usa-se o mesmo tipo de modelo para todas as amostras\n",
        "- Os modelos são treinados independentemente uns dos outros\n",
        "## 3) **Agregação:**\n",
        "- Para fazer previsões, use todos os modelos treinados\n",
        "- Para problemas de classificação: use votação majoritária\n",
        "- Para problemas de regressão: calcule a média das previsões"
      ],
      "metadata": {
        "id": "xQtx36uR9cR9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explicação do Bagging:\n",
        "  Bagging, abreviação de Bootstrap Aggregating, é uma técnica de ensemble learning que visa melhorar a estabilidade e precisão de modelos de aprendizado de máquina. A ideia principal é criar múltiplas versões do conjunto de treinamento usando amostragem com reposição (bootstrap), treinar um modelo separado em cada versão e, em seguida, combinar as previsões desses modelos.\n",
        "\n",
        "  Imagine que você tem um saco de bolinhas coloridas e quer adivinhar a cor predominante. Em vez de olhar para todas as bolinhas de uma vez, você pega um punhado aleatório (com reposição), anota a cor mais comum, e repete isso várias vezes. No final, você decide com base na cor que apareceu mais vezes em todos os seus \"palpites\". Isso é essencialmente o que o Bagging faz, mas com dados e modelos de aprendizado de máquina.\n",
        "  \n",
        "  O Bagging ajuda a reduzir o overfitting, pois cada modelo é treinado em uma amostra ligeiramente diferente dos dados. Isso torna o ensemble final mais robusto a outliers e ruídos nos dados. Além disso, ao combinar múltiplos modelos, o Bagging pode capturar diferentes aspectos dos dados, levando a previsões mais estáveis e precisas."
      ],
      "metadata": {
        "id": "YdND4ntf948c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "Khh_-7FV-XpM"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleBagging:\n",
        "    def __init__(self, n_estimators=10):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.estimators = []\n",
        "\n",
        "    def bootstrap_sample(self, X, y):\n",
        "        n_samples = X.shape[0]\n",
        "        idxs = np.random.choice(n_samples, size=n_samples, replace=True)\n",
        "        return X[idxs], y[idxs]\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.estimators = []\n",
        "        for _ in range(self.n_estimators):\n",
        "            estimator = DecisionTreeClassifier()\n",
        "            X_sample, y_sample = self.bootstrap_sample(X, y)\n",
        "            estimator.fit(X_sample, y_sample)\n",
        "            self.estimators.append(estimator)\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = np.array([estimator.predict(X) for estimator in self.estimators])\n",
        "        return np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=predictions)"
      ],
      "metadata": {
        "id": "oxXAGXOW-iTE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemplo de uso\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "bagging = SimpleBagging(n_estimators=10)\n",
        "bagging.fit(X_train, y_train)\n",
        "\n",
        "predictions = bagging.predict(X_test)\n",
        "accuracy = np.mean(predictions == y_test)\n",
        "print(f\"Acurácia: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2I7aSmX-l6-",
        "outputId": "3d83751e-97ba-44a5-d4a8-a299518f41db"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acurácia: 0.88\n"
          ]
        }
      ]
    }
  ]
}
